#!/usr/bin/env python3
import json
import re
import ssl
import urllib.parse
import urllib.request
import xml.etree.ElementTree as ET
from datetime import datetime
from zoneinfo import ZoneInfo
from pathlib import Path

TZ = ZoneInfo("Asia/Singapore")
OUT = Path("/Users/lin/.openclaw/workspace/web_pages/hotnews/data/news.json")

SOURCES = {
    "ä¸­å›½": ["http://www.people.com.cn/rss/politics.xml"],
    "ç¾å›½ï¼ˆæ¬§ç¾åª’ä½“ï¼‰": [
        "http://rss.cnn.com/rss/edition.rss",
        "https://feeds.bbci.co.uk/news/world/rss.xml"
    ],
    "ä¼Šæ–¯å…°ï¼ˆåŠå²›ç­‰ï¼‰": ["https://www.aljazeera.com/xml/rss/all.xml"],
}

MAX_ITEMS = 30
MAX_CARD_ITEMS = 16
MAX_TRIANGLE = 10
STOPWORDS = {
    "the","a","an","to","of","for","in","on","at","and","or","with","from","is","are",
    "china","chinese","us","u.s","america","american","al","jazeera","says","new","after","over",
    "global","world","news","update","live"
}


def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()


def translate_text(text: str, target: str = "zh-CN"):
    if not text:
        return ""
    try:
        q = urllib.parse.quote(text)
        u = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl={target}&dt=t&q={q}"
        req = urllib.request.Request(u, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=12) as r:
            data = json.loads(r.read().decode("utf-8", errors="ignore"))
        return "".join(part[0] for part in data[0] if part and part[0]).strip() or text
    except Exception:
        return text


def enrich_bilingual(item: dict):
    title = item.get("title", "")
    item["title_en"] = translate_text(title, "en")
    item["title_zh"] = translate_text(title, "zh-CN")
    
    # å¦‚æœæœ‰æè¿°ï¼Œæ¸…ç†HTMLæ ‡ç­¾å¹¶å­˜å‚¨ï¼ˆä¸ç¿»è¯‘ï¼Œé¿å…APIé™åˆ¶ï¼‰
    desc = item.get("description", "")
    if desc:
        # æ¸…ç†HTMLæ ‡ç­¾
        desc_clean = re.sub(r'<[^>]+>', '', desc)
        # æˆªæ–­è¿‡é•¿çš„æè¿°
        if len(desc_clean) > 500:
            desc_clean = desc_clean[:497] + "..."
        item["description"] = desc_clean
        # ä¸ç¿»è¯‘ï¼Œç›´æ¥å­˜å‚¨
    
    return item


def fetch_rss(url: str):
    ctx = ssl.create_default_context()
    req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0 OpenClawNewsBot"})
    with urllib.request.urlopen(req, timeout=20, context=ctx) as r:
        data = r.read()

    root = ET.fromstring(data)
    items = []

    for item in root.findall(".//channel/item"):
        title = clean_text(item.findtext("title"))
        link = clean_text(item.findtext("link"))
        pub = clean_text(item.findtext("pubDate"))
        desc = clean_text(item.findtext("description") or item.findtext("content:encoded") or item.findtext("content"))
        if title and link:
            items.append({"title": title, "link": link, "published": pub, "description": desc})

    if not items:
        ns = {"a": "http://www.w3.org/2005/Atom"}
        for entry in root.findall(".//a:entry", ns):
            title = clean_text(entry.findtext("a:title", default="", namespaces=ns))
            link_el = entry.find("a:link", ns)
            link = clean_text(link_el.attrib.get("href", "") if link_el is not None else "")
            pub = clean_text(entry.findtext("a:updated", default="", namespaces=ns))
            desc = clean_text(entry.findtext("a:summary", default="", namespaces=ns) or entry.findtext("a:content", default="", namespaces=ns))
            if title and link:
                items.append({"title": title, "link": link, "published": pub, "description": desc})

    seen, dedup = set(), []
    for it in items:
        key = (it.get("title", ""), it.get("link", ""))
        if key in seen:
            continue
        seen.add(key)
        dedup.append(enrich_bilingual(it))

    return dedup[:MAX_ITEMS]


def tokenize(title: str):
    words = re.findall(r"[A-Za-z]{3,}", title.lower())
    return {w for w in words if w not in STOPWORDS}


def match_score(a: str, b: str):
    ta, tb = tokenize(a), tokenize(b)
    if not ta or not tb:
        return 0
    return len(ta & tb)


def best_for(base, arr):
    best, bs = None, 0
    for x in arr:
        s = match_score(base["title"], x["title"])
        if s > bs:
            best, bs = x, s
    return best, bs


def extract_key_sentences(text, max_sentences=2):
    """ä»æ–‡æœ¬ä¸­æå–å…³é”®å¥å­ï¼ˆç®€å•ç‰ˆï¼šå–å‰ä¸¤ä¸ªå¥å­ï¼‰"""
    if not text:
        return ""
    # æŒ‰å¥å­åˆ†å‰²ï¼ˆç®€å•åˆ†å‰²ï¼‰
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    return " ".join(sentences[:max_sentences]) + ("..." if len(sentences) > max_sentences else "")


def extract_top_keywords(text, n=5):
    """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼ˆç®€å•ç‰ˆï¼šæŒ‰é¢‘ç‡ï¼‰"""
    if not text:
        return []
    # ç®€å•åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
    words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())
    # è¿‡æ»¤åœç”¨è¯
    stopwords = set(['that', 'with', 'this', 'from', 'have', 'would', 'could', 'should', 'what', 'when', 'where', 'which', 'who', 'whom', 'why', 'how', 'about'])
    words = [w for w in words if w not in stopwords]
    # ç»Ÿè®¡é¢‘ç‡
    from collections import Counter
    freq = Counter(words)
    return [word for word, _ in freq.most_common(n)]


def analyze_sentiment_simple(text):
    """ç®€å•æƒ…æ„Ÿåˆ†æï¼ˆåŸºäºå…³é”®è¯ï¼‰"""
    if not text:
        return "neutral"
    text_lower = text.lower()
    positive_words = ['good', 'great', 'excellent', 'positive', 'success', 'win', 'improve', 'progress', 'achievement']
    negative_words = ['bad', 'poor', 'negative', 'failure', 'loss', 'problem', 'crisis', 'conflict', 'attack']
    
    pos_count = sum(1 for w in positive_words if w in text_lower)
    neg_count = sum(1 for w in negative_words if w in text_lower)
    
    if pos_count > neg_count:
        return "positive"
    elif neg_count > pos_count:
        return "negative"
    else:
        return "neutral"


def ai_view(event):
    # è·å–ä¸‰ä¸ªè§†è§’çš„è¯¦ç»†æ•°æ®
    cn_data = event.get("ä¸­å›½", {}) or {}
    us_data = event.get("ç¾å›½", {}) or {}
    aj_data = event.get("ä¼Šæ–¯å…°", {}) or {}
    
    # è·å–æè¿°ï¼ˆä¼˜å…ˆä½¿ç”¨descriptionï¼Œæ²¡æœ‰åˆ™ç”¨æ ‡é¢˜ï¼‰
    cn_desc = cn_data.get("description_zh") or cn_data.get("description") or cn_data.get("title_zh", "")
    us_desc = us_data.get("description_zh") or us_data.get("description") or us_data.get("title_zh", "")
    aj_desc = aj_data.get("description_zh") or aj_data.get("description") or aj_data.get("title_zh", "")
    
    # è·å–æ ‡é¢˜ç”¨äºåˆ†ç±»
    cn_title = cn_data.get("title_zh", "") or cn_data.get("title", "")
    us_title = us_data.get("title_zh", "") or us_data.get("title", "")
    aj_title = aj_data.get("title_zh", "") or aj_data.get("title", "")
    combined_titles = f"{cn_title} {us_title} {aj_title}"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æè¿°å†…å®¹å¯ç”¨äºè¯¦ç»†æ¯”è¾ƒï¼ˆè‡³å°‘ä¸€ä¸ªè§†è§’æœ‰è¯¦ç»†æè¿°ï¼‰
    has_detailed_descriptions = (len(cn_desc) > 50 or len(us_desc) > 50 or len(aj_desc) > 50)
    
    # åŸºç¡€åˆ†ææ¡†æ¶
    base_analysis = ""
    
    # è´¢ç»ç»æµç±»
    if any(k in combined_titles for k in ["å…³ç¨", "è´¸æ˜“", "ç»æµ", "é€šèƒ€", "å¸‚åœº", "è‚¡å¸‚", "è´¢æ”¿", "åˆ©ç‡", "é“¶è¡Œ", "æŠ•èµ„", "è´§å¸", "æ±‡ç‡"]):
        base_analysis = (
            "ğŸ“Š æˆ‘çš„æ·±åº¦åˆ†æï¼ˆè´¢ç»ç±»ï¼‰ï¼š\n\n"
            "1. **äº‹å®å±‚éªŒè¯**ï¼šé¦–å…ˆéœ€è¦åŒºåˆ†æ˜¯æ”¿ç­–ææ¡ˆã€å®˜æ–¹å£°æ˜è¿˜æ˜¯å·²ç«‹æ³•ç”Ÿæ•ˆã€‚ä¸­å›½åª’ä½“é€šå¸¸æŠ¥é“æ”¿ç­–æ¡†æ¶ä¸å®è§‚ç›®æ ‡ï¼Œ"
            "æ¬§ç¾åª’ä½“èšç„¦å¸‚åœºååº”ä¸èµ„æœ¬æµåŠ¨ï¼ŒåŠå²›è§†è§’å¯èƒ½å…³æ³¨å¯¹å‘å±•ä¸­å›½å®¶è´¸æ˜“çš„å½±å“ã€‚\n\n"
            "2. **é¢„æœŸå±‚è§£æ„**ï¼šæ ‡é¢˜æƒ…ç»ªå¸¸æœ‰æ”¾å¤§æ•ˆåº”ã€‚å»ºè®®åŒæ—¶æŸ¥çœ‹å€ºåˆ¸æ”¶ç›Šç‡ï¼ˆç¾å€º10å¹´æœŸï¼‰ã€ç¾å…ƒæŒ‡æ•°ï¼ˆDXYï¼‰ã€"
            "åŸæ²¹ä»·æ ¼ï¼ˆBrentï¼‰çš„å³æ—¶å˜åŠ¨ï¼Œè¿™ä¸‰é¡¹æ˜¯åˆ¤æ–­è´¢ç»æ–°é—»çœŸå®å½±å“çš„é¢†å…ˆæŒ‡æ ‡ã€‚\n\n"
            "3. **æ‰§è¡Œå±‚è·Ÿè¸ª**ï¼šæœ€ç»ˆè¦çœ‹ä¼ä¸šè´¢æŠ¥ä¸­çš„æˆæœ¬å˜åŒ–ã€äº§ä¸šé“¾è°ƒç ”ä¸­çš„è®¢å•æ•°æ®ã€ä»¥åŠå›½é™…èµ„é‡‘æµå‘æŠ¥å‘Šã€‚"
            "å•ä¸€åª’ä½“æŠ¥é“å¾€å¾€ç¼ºå°‘è¿™äº›åç»­éªŒè¯ç¯èŠ‚ã€‚\n\n"
            "å»ºè®®æ“ä½œï¼šå¯¹è´¢ç»æ–°é—»ä¿æŒ48å°æ—¶è§‚å¯ŸæœŸï¼Œç­‰å¾…å¸‚åœºæ¶ˆåŒ–ä¸æ›´å¤šæ•°æ®æŠ«éœ²åå†åšåˆ¤æ–­ã€‚"
        )
        category = "è´¢ç»ç»æµ"

    # åœ°ç¼˜å†²çªç±»
    elif any(k in combined_titles for k in ["å†²çª", "æˆ˜äº‰", "è¢­å‡»", "åœç«", "å¯¼å¼¹", "å†›äº‹", "è¾¹å¢ƒ", "äººé“", "æ­¦å™¨", "é˜²å¾¡", "å†›é˜Ÿ", "å£«å…µ"]):
        base_analysis = (
            "âš”ï¸ æˆ‘çš„æ·±åº¦åˆ†æï¼ˆåœ°ç¼˜å†²çªç±»ï¼‰ï¼š\n\n"
            "1. **å™äº‹æ¡†æ¶å·®å¼‚**ï¼šä¸­å›½æŠ¥é“å¼ºè°ƒå¤šè¾¹å¤–äº¤ä¸åœ°åŒºç¨³å®šæ¡†æ¶ï¼Œå¸¸å¼•è¿°å®˜æ–¹ç«‹åœºä¸è”åˆå›½å†³è®®ï¼›"
            "æ¬§ç¾æŠ¥é“ä¾§é‡æˆ˜ç•¥åšå¼ˆã€ç›Ÿå‹åè°ƒä¸å®‰å…¨å¨èƒè¯„ä¼°ï¼›åŠå²›è§†è§’èšç„¦å¹³æ°‘ä¼¤äº¡ã€äººé“å±æœºä¸ç°åœºçºªå®ã€‚\n\n"
            "2. **äº‹å®äº¤å‰éªŒè¯**ï¼šå»ºè®®åˆ¶ä½œæ—¶é—´çº¿å¯¹æ¯”è¡¨ï¼Œæ ‡æ³¨ä¸‰æ–¹éƒ½ç¡®è®¤çš„æ ¸å¿ƒäº‹å®ï¼ˆæ—¶é—´ã€åœ°ç‚¹ã€ä¼¤äº¡æ•°å­—ã€æ­¦å™¨ç±»å‹ï¼‰ï¼Œ"
            "å†å•ç‹¬åˆ—å‡ºå„æ–¹ç‹¬æœ‰çš„è¡¥å……ä¿¡æ¯æˆ–è§£è¯»è§’åº¦ã€‚\n\n"
            "3. **å½±å“è¯„ä¼°çŸ©é˜µ**ï¼šçŸ­æœŸçœ‹èˆªè¿æŒ‡æ•°ï¼ˆBDIï¼‰ã€åŸæ²¹æœŸè´§ï¼ˆWTI/Brentï¼‰ã€é»„é‡‘ä»·æ ¼ï¼›"
            "ä¸­æœŸçœ‹ç›¸å…³å›½å®¶ä¸»æƒCDSåˆ©å·®ã€è´§å¸æ±‡ç‡æ³¢åŠ¨ï¼›é•¿æœŸçœ‹åŒºåŸŸä¾›åº”é“¾é‡æ„å¯èƒ½ã€‚\n\n"
            "è­¦æƒ•ç‚¹ï¼šé¿å…è¿‡æ—©æ¥å—å•ä¸€å½’å› å™äº‹ï¼Œå†²çªäº‹ä»¶å¾€å¾€æœ‰å¤æ‚çš„å†å²è„‰ç»œä¸ä»£ç†æˆ˜äº‰èƒŒæ™¯ã€‚"
        )
        category = "åœ°ç¼˜å†²çª"

    # ç§‘æŠ€å¤ªç©ºç±»
    elif any(k in combined_titles for k in ["NASA", "å¤ªç©º", "å®‡èˆª", "å«æ˜Ÿ", "ç«ç®­", "èˆªå¤©", "æœˆçƒ", "ç«æ˜Ÿ", "æ¢ç´¢", "ç§‘æŠ€", "åˆ›æ–°", "äººå·¥æ™ºèƒ½", "AI"]):
        base_analysis = (
            "ğŸš€ æˆ‘çš„æ·±åº¦åˆ†æï¼ˆç§‘æŠ€å¤ªç©ºç±»ï¼‰ï¼š\n\n"
            "1. **æŠ¥é“è§’åº¦å¯¹æ¯”**ï¼šä¸­å›½åª’ä½“çªå‡ºå›½å®¶ç§‘æŠ€æˆå°±ä¸å·¥ç¨‹çªç ´ï¼Œå¼ºè°ƒè‡ªä¸»åˆ›æ–°ä¸å›¢é˜Ÿåä½œï¼›"
            "æ¬§ç¾åª’ä½“æ³¨é‡æŠ€æœ¯ç»†èŠ‚ã€å•†ä¸šåº”ç”¨ä¸å›½é™…ç«äº‰ï¼›åŠå²›è§†è§’å¯èƒ½å…³æ³¨ç§‘æŠ€ä¼¦ç†ã€å…¨çƒåˆä½œä¸å‘å±•ä¸­å›½å®¶å‚ä¸ã€‚\n\n"
            "2. **æŠ€æœ¯æˆç†Ÿåº¦è¯„ä¼°**ï¼šåŒºåˆ†æ¦‚å¿µéªŒè¯ã€åŸå‹æµ‹è¯•ã€æ­£å¼éƒ¨ç½²ç­‰é˜¶æ®µã€‚å¤ªç©ºä»»åŠ¡å°¤å…¶è¦å…³æ³¨å‘å°„çª—å£ã€"
            "ä»»åŠ¡æ—¶é•¿ã€æœ‰æ•ˆè½½è·æˆåŠŸç‡ç­‰ç¡¬æŒ‡æ ‡ï¼Œè€Œéä»…ä»…å®£ä¼ å£å¾„ã€‚\n\n"
            "3. **äº§ä¸šé“¾å½±å“**ï¼šèˆªå¤©ç§‘æŠ€å¸¦åŠ¨æ–°ææ–™ã€é€šä¿¡ã€å¯¼èˆªã€é¥æ„Ÿç­‰å¤šä¸ªäº§ä¸šé“¾ã€‚"
            "å»ºè®®è·Ÿè¸ªç›¸å…³ä¸Šå¸‚å…¬å¸è¡¨ç°ã€ä¸“åˆ©å‘å¸ƒé¢‘ç‡ã€å›½é™…åˆä½œåè®®ç­¾ç½²æƒ…å†µã€‚\n\n"
            "è§‚å¯Ÿå»ºè®®ï¼šç§‘æŠ€æ–°é—»éœ€ç»“åˆä¸“ä¸šæœŸåˆŠè®ºæ–‡ä¸å·¥ç¨‹å¸ˆç¤¾ç¾¤è®¨è®ºï¼Œé¿å…ä»…ä¾èµ–å¤§ä¼—åª’ä½“æŠ¥é“ã€‚"
        )
        category = "ç§‘æŠ€å¤ªç©º"

    # æ”¿æ²»å¤–äº¤ç±»
    elif any(k in combined_titles for k in ["å¤–äº¤", "è®¿é—®", "ä¼šè°ˆ", "åè®®", "æ¡çº¦", "å³°ä¼š", "è”åˆå›½", "åˆ¶è£", "æŠ—è®®", "é€‰ä¸¾", "æ€»ç»Ÿ", "æ€»ç†"]):
        base_analysis = (
            "ğŸ›ï¸ æˆ‘çš„æ·±åº¦åˆ†æï¼ˆæ”¿æ²»å¤–äº¤ç±»ï¼‰ï¼š\n\n"
            "1. **è®®ç¨‹è®¾ç½®åˆ†æ**ï¼šä¸­å›½æŠ¥é“å¼ºè°ƒåŒè¾¹å…³ç³»ä¸åŠ¡å®åˆä½œï¼Œèšç„¦å…·ä½“æˆæœä¸å…±è¯†æ–‡ä»¶ï¼›"
            "æ¬§ç¾åª’ä½“å…³æ³¨æƒåŠ›åŠ¨æ€ã€æˆ˜ç•¥æ„å›¾ä¸æ½œåœ¨æ‘©æ“¦ç‚¹ï¼›åŠå²›è§†è§’å¸¸ä»å…¨çƒå—æ–¹è§†è§’åˆ†ææƒåŠ›å¹³è¡¡å˜åŒ–ã€‚\n\n"
            "2. **ä¿¡å·è§£è¯»å±‚æ¬¡**ï¼šè¡¨å±‚æ˜¯å®˜æ–¹å£°æ˜ä¸ç¤¼ä»ªå®‰æ’ï¼Œä¸­å±‚æ˜¯éšè¡Œäººå‘˜çº§åˆ«ä¸è®®ç¨‹æ—¶é•¿ï¼Œ"
            "æ·±å±‚æ˜¯åç»­æ”¿ç­–è°ƒæ•´ä¸èµ„é‡‘æµå‘ã€‚å¤–äº¤æ–°é—»éœ€è¦å¤šå±‚æ¬¡ä¿¡å·äº¤å‰éªŒè¯ã€‚\n\n"
            "3. **å†å²å‚ç…§ç³»**ï¼šå½“å‰äº‹ä»¶åº”æ”¾åœ¨è‡³å°‘10å¹´åŒè¾¹å…³ç³»è„‰ç»œä¸­ç†è§£ï¼Œ"
            "å…³æ³¨æ­¤å‰ç±»ä¼¼æƒ…å¢ƒä¸‹çš„å„æ–¹ååº”æ¨¡å¼ä¸æœ€ç»ˆç»“æœã€‚\n\n"
            "å…³é”®æé†’ï¼šæ”¿æ²»å¤–äº¤æŠ¥é“æœ€æ˜“å—æ„è¯†å½¢æ€æ»¤é•œå½±å“ï¼Œå»ºè®®åŒæ—¶æŸ¥é˜…å„æ–¹æ™ºåº“ç®€æŠ¥ä¸å­¦æœ¯åˆ†æã€‚"
        )
        category = "æ”¿æ²»å¤–äº¤"

    # ç¤¾ä¼šæ°‘ç”Ÿç±»
    elif any(k in combined_titles for k in ["æ°‘ç”Ÿ", "æ•™è‚²", "åŒ»ç–—", "å¥åº·", "ä½æˆ¿", "å°±ä¸š", "æ”¶å…¥", "æ¶ˆè´¹", "å…»è€", "ç¤¾ä¿", "ç¦åˆ©", "äººå£"]):
        base_analysis = (
            "ğŸ¥ æˆ‘çš„æ·±åº¦åˆ†æï¼ˆç¤¾ä¼šæ°‘ç”Ÿç±»ï¼‰ï¼š\n\n"
            "1. **æ”¿ç­–è½åœ°å·®å¼‚**ï¼šä¸­å›½æŠ¥é“ä¾§é‡æ”¿ç­–å‡ºå°ä¸è¯•ç‚¹æ•ˆæœï¼Œå¼ºè°ƒæ”¿åºœæŠ•å…¥ä¸è¦†ç›–ç‡æå‡ï¼›"
            "æ¬§ç¾åª’ä½“å…³æ³¨ä¸ªä½“æ¡ˆä¾‹ã€åˆ¶åº¦æ¯”è¾ƒä¸ç¤¾ä¼šå…¬å¹³ï¼›åŠå²›è§†è§’å¯èƒ½èšç„¦å…¨çƒä¸å¹³ç­‰ä¸èµ„æºåˆ†é…ã€‚\n\n"
            "2. **æ•°æ®æºå¯¹æ¯”**ï¼šæ°‘ç”Ÿè®®é¢˜éœ€è¦å…·ä½“æ•°æ®æ”¯æŒã€‚å»ºè®®å¯¹æ¯”å®˜æ–¹ç»Ÿè®¡æ•°æ®ã€å­¦æœ¯è°ƒç ”æŠ¥å‘Šã€"
            "å›½é™…ç»„ç»‡è¯„ä¼°ä¸æ°‘é—´è°ƒæŸ¥ï¼Œæ³¨æ„ç»Ÿè®¡å£å¾„ä¸æ ·æœ¬ä»£è¡¨æ€§çš„å·®å¼‚ã€‚\n\n"
            "3. **é•¿æœŸè¶‹åŠ¿è§‚å¯Ÿ**ï¼šç¤¾ä¼šæ”¿ç­–æ•ˆæœå¾€å¾€æœ‰3-5å¹´æ»åæœŸã€‚å…³æ³¨ç›¸å…³é¢†åŸŸçš„å­¦æœ¯è®ºæ–‡å‘è¡¨ã€"
            "NGOè¯„ä¼°æŠ¥å‘Šã€ä»¥åŠå—å½±å“ç¾¤ä½“çš„é•¿æœŸè¿½è¸ªç ”ç©¶ã€‚\n\n"
            "åˆ†æå»ºè®®ï¼šé¿å…ä»…å‡­çŸ­æœŸåª’ä½“æŠ¥é“åˆ¤æ–­é•¿æœŸç¤¾ä¼šè¶‹åŠ¿ï¼Œæ°‘ç”Ÿè®®é¢˜éœ€è¦è€å¿ƒä¸å¤šç»´æ•°æ®ã€‚"
        )
        category = "ç¤¾ä¼šæ°‘ç”Ÿ"

    # é»˜è®¤ç»¼åˆç±»
    else:
        base_analysis = (
            "ğŸ” æˆ‘çš„æ·±åº¦åˆ†æï¼ˆç»¼åˆç±»ï¼‰ï¼š\n\n"
            "1. **ä¿¡æ¯çŸ©é˜µæ„å»º**ï¼šå»ºè®®åˆ›å»ºä¸‰æ–¹æŠ¥é“å¯¹æ¯”è¡¨æ ¼ï¼Œåˆ—å‡ºäº‹ä»¶æ ¸å¿ƒè¦ç´ ï¼ˆæ—¶é—´ã€åœ°ç‚¹ã€ä¸»ä½“ã€è¡ŒåŠ¨ã€ç»“æœï¼‰ï¼Œ"
            "å†æ ‡æ³¨å„æ–¹ç‹¬æœ‰çš„èƒŒæ™¯è¡¥å……ã€å› æœè§£é‡Šä¸ä»·å€¼åˆ¤æ–­ã€‚\n\n"
            "2. **ä¿¡æºå¯é æ€§è¯„ä¼°**ï¼šæŸ¥éªŒå„æ–¹å¼•ç”¨çš„åŸå§‹ä¿¡æºï¼ˆå®˜æ–¹æ–‡ä»¶ã€ç°åœºå½±åƒã€ä¸“å®¶è®¿è°ˆã€æ•°æ®æŠ¥å‘Šï¼‰ï¼Œ"
            "æ³¨æ„åŒ¿åä¿¡æºä¸æ˜ç¡®ç½²åçš„å·®å¼‚ï¼Œä»¥åŠä¿¡æºçš„æ—¶é—´æˆ³ä¸åœ°ç†ä½ç½®ã€‚\n\n"
            "3. **è®¤çŸ¥åå·®è¯†åˆ«**ï¼šè­¦æƒ•ç¡®è®¤åè¯¯ï¼ˆåªå…³æ³¨æ”¯æŒè‡ªå·±è§‚ç‚¹çš„æŠ¥é“ï¼‰ã€å¯å¾—æ€§åè¯¯ï¼ˆè¿‡åº¦ä¾èµ–æœ€æ˜“è·å–çš„ä¿¡æ¯ï¼‰ã€"
            "æ¡†æ¶æ•ˆåº”ï¼ˆåŒä¸€äº‹å®ä¸åŒè¡¨è¿°å¯¼è‡´ä¸åŒåˆ¤æ–­ï¼‰ã€‚\n\n"
            "æœ€ç»ˆå»ºè®®ï¼šé‡è¦äº‹ä»¶åº”ç­‰å¾…24-48å°æ—¶ï¼Œå¾…æ›´å¤šä¿¡æ¯æµ®ç°ä¸äº‹å®æ ¸æŸ¥å®Œæˆåå†å½¢æˆç¨³å®šåˆ¤æ–­ã€‚"
        )
        category = "ç»¼åˆ"
    
    # å¦‚æœæœ‰è¯¦ç»†æè¿°ï¼Œæ·»åŠ å…·ä½“å†…å®¹æ¯”è¾ƒ
    if has_detailed_descriptions:
        detailed_comparison = f"\n\nğŸ“ **åŸºäºä¸‰æ–¹æŠ¥é“å†…å®¹çš„è¯¦ç»†æ¯”è¾ƒï¼ˆ{category}ç±»ï¼‰**\n\n"
        
        # æ”¶é›†å„è§†è§’çš„æ•°æ®ç”¨äºåˆ†æ
        perspectives = []
        if cn_desc:
            perspectives.append(("ğŸ‡¨ğŸ‡³ ä¸­å›½", cn_desc))
        if us_desc:
            perspectives.append(("ğŸ‡ºğŸ‡¸ æ¬§ç¾", us_desc))
        if aj_desc:
            perspectives.append(("ğŸŒ ä¼Šæ–¯å…°", aj_desc))
        
        # æ˜¾ç¤ºå„è§†è§’ç„¦ç‚¹
        for label, desc in perspectives:
            key_sentences = extract_key_sentences(desc)
            detailed_comparison += f"{label} **æŠ¥é“ç„¦ç‚¹**ï¼š{key_sentences}\n\n"
        
        # æ–‡æœ¬åˆ†æéƒ¨åˆ†
        detailed_comparison += "ğŸ” **æ·±åº¦æ–‡æœ¬åˆ†æ**ï¼š\n"
        
        # 1. æŠ¥é“é•¿åº¦å¯¹æ¯”
        length_info = []
        for label, desc in perspectives:
            word_count = len(desc.split())
            length_info.append(f"{label}: {word_count}è¯")
        if length_info:
            detailed_comparison += f"1. **æŠ¥é“é•¿åº¦**ï¼š{' | '.join(length_info)}\n"
        
        # 2. å…³é”®è¯å¯¹æ¯”
        all_keywords = []
        for label, desc in perspectives:
            keywords = extract_top_keywords(desc, 3)
            if keywords:
                all_keywords.append(f"{label}: {', '.join(keywords)}")
        if all_keywords:
            detailed_comparison += f"2. **å…³é”®è¯**ï¼š{' | '.join(all_keywords)}\n"
        
        # 3. ç®€å•æƒ…æ„Ÿåˆ†æ
        sentiments = []
        for label, desc in perspectives:
            sentiment = analyze_sentiment_simple(desc)
            sentiment_map = {"positive": "åæ­£é¢", "negative": "åè´Ÿé¢", "neutral": "ä¸­æ€§"}
            sentiments.append(f"{label}: {sentiment_map[sentiment]}")
        if sentiments:
            detailed_comparison += f"3. **æƒ…æ„Ÿå€¾å‘**ï¼š{' | '.join(sentiments)}\n"
        
        # 4. æŠ¥é“è§’åº¦å·®å¼‚ï¼ˆåŒæ—¶æ£€æŸ¥ä¸­è‹±æ–‡å…³é”®è¯ï¼‰
        angles = []
        cn_lower = cn_desc.lower() if cn_desc else ""
        us_lower = us_desc.lower() if us_desc else ""
        aj_lower = aj_desc.lower() if aj_desc else ""
        
        if any(k in cn_lower for k in ["å‘å±•", "åˆä½œ", "ç¨³å®š", "development", "cooperation", "stability", "progress"]):
            angles.append("ä¸­å›½æŠ¥é“å¼ºè°ƒå‘å±•ä¸ç¨³å®šæ¡†æ¶")
        if any(k in us_lower for k in ["å¸‚åœº", "ç»æµ", "é£é™©", "market", "economy", "risk", "investment", "financial"]):
            angles.append("æ¬§ç¾æŠ¥é“å…³æ³¨å¸‚åœºä¸é£é™©è¯„ä¼°")
        if any(k in aj_lower for k in ["äººé“", "å¹³æ°‘", "ç°åœº", "humanitarian", "civilian", "on the ground", "victim", "crisis"]):
            angles.append("åŠå²›æŠ¥é“èšç„¦äººé“ä¸ç°åœºç»†èŠ‚")
        
        if angles:
            detailed_comparison += f"4. **æŠ¥é“è§’åº¦**ï¼š{'ï¼›'.join(angles)}ã€‚\n"
        else:
            detailed_comparison += "4. **æŠ¥é“è§’åº¦**ï¼šä¸‰æ–¹å‡ä»å„è‡ªå¸¸è§„æ¡†æ¶æŠ¥é“æ­¤äº‹ã€‚\n"
        
        # 5. äº‹å®ä¾§é‡å·®å¼‚
        facts = []
        if any(word in cn_lower for word in ["æ”¿ç­–", "æªæ–½", "å†³å®š", "å®£å¸ƒ", "policy", "measure", "decision", "announce"]):
            facts.append("ä¸­å›½æŠ¥é“ä¾§é‡æ”¿ç­–å±‚é¢")
        if any(word in us_lower for word in ["å½±å“", "ååº”", "åˆ†æ", "é¢„æµ‹", "impact", "effect", "analysis", "predict", "response"]):
            facts.append("æ¬§ç¾æŠ¥é“ä¾§é‡å½±å“åˆ†æ")
        if any(word in aj_lower for word in ["ç°åœº", "ä¼¤äº¡", "å±æœº", "å›°éš¾", "on site", "casualty", "crisis", "difficulty", "suffering"]):
            facts.append("åŠå²›æŠ¥é“ä¾§é‡ç°åœºæƒ…å†µ")
        
        if facts:
            detailed_comparison += f"5. **äº‹å®ä¾§é‡**ï¼š{'ï¼›'.join(facts)}ã€‚\n"
        else:
            detailed_comparison += "5. **äº‹å®ä¾§é‡**ï¼šåŸºäºç°æœ‰æè¿°ï¼Œä¸‰æ–¹æŠ¥é“çš„äº‹å®ä¾§é‡å·®å¼‚ä¸æ˜æ˜¾ã€‚\n"
        
        # 6. ç»¼åˆå»ºè®®
        detailed_comparison += "6. **é˜…è¯»å»ºè®®**ï¼šç»¼åˆä¸‰æ–¹å†…å®¹å¯è·å¾—æ›´å®Œæ•´å›¾æ™¯â€”â€”ä¸­å›½è§†è§’æä¾›æ”¿ç­–æ¡†æ¶ï¼Œæ¬§ç¾è§†è§’æä¾›é£é™©åˆ†æï¼ŒåŠå²›è§†è§’æä¾›åœ°é¢ç°å®ã€‚\n"
        
        return base_analysis + detailed_comparison
    else:
        # æ²¡æœ‰è¯¦ç»†æè¿°ï¼Œè¿”å›åŸºç¡€åˆ†æ
        return base_analysis


def summary_view(event):
    c = event.get("ä¸­å›½")
    u = event.get("ç¾å›½")
    i = event.get("ä¼Šæ–¯å…°")
    
    # æå–å…³é”®ä¿¡æ¯ç”¨äºä¸ªæ€§åŒ–æ€»ç»“
    event_title = event.get("event_hint_zh", "") or event.get("event_hint", "")
    categories = []
    if any(k in event_title for k in ["å…³ç¨", "è´¸æ˜“", "ç»æµ", "è‚¡å¸‚", "è´¢æ”¿", "é“¶è¡Œ", "è´§å¸"]):
        categories.append("è´¢ç»ç»æµ")
    if any(k in event_title for k in ["å†²çª", "æˆ˜äº‰", "å†›äº‹", "è¢­å‡»", "é˜²å¾¡", "æ­¦å™¨"]):
        categories.append("åœ°ç¼˜å®‰å…¨")
    if any(k in event_title for k in ["NASA", "å¤ªç©º", "å®‡èˆª", "ç§‘æŠ€", "äººå·¥æ™ºèƒ½", "AI", "åˆ›æ–°"]):
        categories.append("ç§‘æŠ€åˆ›æ–°")
    if any(k in event_title for k in ["å¤–äº¤", "åè®®", "æ¡çº¦", "å³°ä¼š", "è”åˆå›½", "åˆ¶è£", "é€‰ä¸¾"]):
        categories.append("æ”¿æ²»å¤–äº¤")
    if any(k in event_title for k in ["æ°‘ç”Ÿ", "æ•™è‚²", "åŒ»ç–—", "å¥åº·", "ä½æˆ¿", "å°±ä¸š", "å…»è€"]):
        categories.append("ç¤¾ä¼šæ°‘ç”Ÿ")
    
    category_str = "ã€".join(categories) if categories else "ç»¼åˆ"
    
    parts = []
    parts.append(f"ğŸ”¬ ä¸‰æ–¹è§†è§’æ·±åº¦æ€»ç»“ï¼ˆ{category_str}ç±»äº‹ä»¶ï¼‰\n\n")
    
    # ç»Ÿè®¡å„è§†è§’å­˜åœ¨æƒ…å†µ
    perspective_count = sum(1 for x in [c, u, i] if x)
    
    parts.append(f"ğŸ“Š **æœ¬äº‹ä»¶è¦†ç›–æƒ…å†µ**ï¼š{perspective_count}/3 ä¸ªè§†è§’æŠ¥é“ï¼ˆ{['ä¸­å›½','ç¾å›½','ä¼Šæ–¯å…°'][:perspective_count]}ï¼‰\n\n")
    
    if c:
        # åˆ†æä¸­å›½æŠ¥é“ç‰¹ç‚¹ï¼ˆåŸºäºæè¿°å¦‚æœå­˜åœ¨ï¼‰
        cn_desc = c.get("description", "")
        focus_areas = []
        if any(k in cn_desc.lower() for k in ["å‘å±•", "è¿›æ­¥", "åˆä½œ", "ç¨³å®š"]):
            focus_areas.append("å‘å±•ç¨³å®š")
        if any(k in cn_desc.lower() for k in ["æ”¿ç­–", "æªæ–½", "å†³å®š", "è§„åˆ’"]):
            focus_areas.append("æ”¿ç­–è§„åˆ’")
        if any(k in cn_desc.lower() for k in ["æŠ€æœ¯", "åˆ›æ–°", "çªç ´", "æˆå°±"]):
            focus_areas.append("æŠ€æœ¯åˆ›æ–°")
        
        focus_str = f"ï¼ˆé‡ç‚¹å…³æ³¨ï¼š{'ã€'.join(focus_areas)}ï¼‰" if focus_areas else ""
        parts.append(f"ğŸ‡¨ğŸ‡³ **ä¸­å›½è§†è§’**{focus_str}ï¼šé€šå¸¸èšç„¦æ”¿ç­–æ¡†æ¶ã€é•¿æœŸè§„åˆ’ã€ç¤¾ä¼šç¨³å®šä¸é›†ä½“æˆå°±ï¼›æŠ¥é“é£æ ¼ç¨³é‡ï¼Œ"
                    "å¼ºè°ƒåˆ¶åº¦ä¼˜åŠ¿ä¸æ²»ç†æ•ˆèƒ½ï¼›åœ¨æŠ€æœ¯ç±»æ–°é—»ä¸­çªå‡ºè‡ªä¸»åˆ›æ–°ï¼Œåœ¨å¤–äº¤æ–°é—»ä¸­å¼ºè°ƒåˆä½œå…±èµ¢ã€‚\n\n")
    
    if u:
        # åˆ†ææ¬§ç¾æŠ¥é“ç‰¹ç‚¹
        us_desc = u.get("description", "")
        focus_areas = []
        if any(k in us_desc.lower() for k in ["market", "economy", "financial", "investment"]):
            focus_areas.append("å¸‚åœºç»æµ")
        if any(k in us_desc.lower() for k in ["risk", "challenge", "problem", "threat"]):
            focus_areas.append("é£é™©æŒ‘æˆ˜")
        if any(k in us_desc.lower() for k in ["analysis", "impact", "effect", "consequence"]):
            focus_areas.append("å½±å“åˆ†æ")
        
        focus_str = f"ï¼ˆé‡ç‚¹å…³æ³¨ï¼š{'ã€'.join(focus_areas)}ï¼‰" if focus_areas else ""
        parts.append(f"ğŸ‡ºğŸ‡¸ **æ¬§ç¾è§†è§’**{focus_str}ï¼šä¾§é‡ä¸ªä½“æƒåˆ©ã€åˆ¶åº¦åˆ¶è¡¡ã€å¸‚åœºç«äº‰ä¸æˆ˜ç•¥åšå¼ˆï¼›æŠ¥é“å¸¸é‡‡ç”¨æ‰¹åˆ¤æ€§è´¨ç–‘è§’åº¦ï¼Œ"
                    "å…³æ³¨æƒåŠ›åŠ¨æ€ä¸æ½œåœ¨é£é™©ï¼›åœ¨è´¢ç»æ–°é—»ä¸­å¼ºè°ƒå¸‚åœºååº”ï¼Œåœ¨åœ°ç¼˜æ–°é—»ä¸­åˆ†æå®‰å…¨å½±å“ã€‚\n\n")
    
    if i:
        # åˆ†æä¼Šæ–¯å…°æŠ¥é“ç‰¹ç‚¹
        aj_desc = i.get("description", "")
        focus_areas = []
        if any(k in aj_desc.lower() for k in ["humanitarian", "civilian", "people", "victim"]):
            focus_areas.append("äººé“å…³æ€€")
        if any(k in aj_desc.lower() for k in ["on the ground", "site", "location", "scene"]):
            focus_areas.append("ç°åœºç»†èŠ‚")
        if any(k in aj_desc.lower() for k in ["crisis", "suffering", "difficulty", "challenge"]):
            focus_areas.append("å±æœºå›°éš¾")
        
        focus_str = f"ï¼ˆé‡ç‚¹å…³æ³¨ï¼š{'ã€'.join(focus_areas)}ï¼‰" if focus_areas else ""
        parts.append(f"ğŸŒ **ä¼Šæ–¯å…°/åŠå²›è§†è§’**{focus_str}ï¼šå¾€å¾€ä»å…¨çƒå—æ–¹ä¸å‘å±•ä¸­å›½å®¶ç«‹åœºå‡ºå‘ï¼Œå…³æ³¨ç°åœºç»†èŠ‚ã€"
                    "äººé“åæœä¸æƒåŠ›ä¸å¹³ç­‰ï¼›æŠ¥é“é£æ ¼æ›´å…·å™äº‹æ€§ï¼Œå¼ºè°ƒæ™®é€šäººçš„ç»å†ä¸æƒ…æ„Ÿï¼›"
                    "å¸¸ä¸ºè¥¿æ–¹ä¸»æµå™äº‹æä¾›é‡è¦çš„è¡¥å……ä¸åˆ¶è¡¡è§†è§’ã€‚\n\n")
    
    # åŸºäºå®é™…å†…å®¹çš„å»ºè®®
    parts.append("ğŸ“ˆ **åŸºäºæœ¬äº‹ä»¶å†…å®¹çš„åˆ†æå»ºè®®**ï¼š\n")
    
    advice_points = []
    if c and ("æ”¿ç­–" in str(c.get("description", "")).lower() or "policy" in str(c.get("description", "")).lower()):
        advice_points.append("ä»ä¸­å›½æŠ¥é“ä¸­ç†è§£æ”¿ç­–æ„å›¾ä¸å®æ–½æ¡†æ¶")
    
    if u and any(k in str(u.get("description", "")).lower() for k in ["impact", "effect", "risk", "market"]):
        advice_points.append("ä»æ¬§ç¾æŠ¥é“ä¸­è¯„ä¼°æ½œåœ¨å½±å“ä¸é£é™©å˜é‡")
    
    if i and any(k in str(i.get("description", "")).lower() for k in ["human", "civilian", "ground", "site"]):
        advice_points.append("ä»åŠå²›æŠ¥é“ä¸­æ„Ÿå—ç°åœºç°å®ä¸äººæ–‡ç»´åº¦")
    
    if not advice_points:
        advice_points = [
            "ç”¨ä¸­å›½è§†è§’ç†è§£æ”¿ç­–æ„å›¾ä¸é•¿æœŸæ¡†æ¶",
            "ç”¨æ¬§ç¾è§†è§’è¯„ä¼°å¸‚åœºååº”ä¸é£é™©å˜é‡", 
            "ç”¨åŠå²›è§†è§’æ„Ÿå—ç°åœºç°å®ä¸äººæ–‡ç»´åº¦"
        ]
    
    for idx, point in enumerate(advice_points, 1):
        # ç§»é™¤å¯èƒ½å·²ç»å­˜åœ¨çš„ç¼–å·
        point_clean = point[3:] if point[:3] in ["1. ", "2. ", "3. ", "4. ", "5. "] else point
        parts.append(f"{idx}. {point_clean}\n")
    
    parts.append("\n")
    
    # æœ€ç»ˆæ´å¯Ÿï¼ˆæ ¹æ®äº‹ä»¶ç±»å‹è°ƒæ•´ï¼‰
    if "è´¢ç»ç»æµ" in categories:
        parts.append("ğŸ’¡ **è´¢ç»äº‹ä»¶æ´å¯Ÿ**ï¼šæ”¿ç­–å£°æ˜ä¸å¸‚åœºååº”å¸¸æœ‰æ—¶é—´å·®ï¼Œå»ºè®®å…³æ³¨åç»­48å°æ—¶çš„å®é™…æ•°æ®éªŒè¯ã€‚\n")
    elif "åœ°ç¼˜å®‰å…¨" in categories:
        parts.append("ğŸ’¡ **åœ°ç¼˜äº‹ä»¶æ´å¯Ÿ**ï¼šå†²çªæŠ¥é“æœ€æ˜“å—å™äº‹æ¡†æ¶å½±å“ï¼Œé‡ç‚¹åŒºåˆ†äº‹å®é™ˆè¿°ä¸å½’å› è§£é‡Šã€‚\n")
    elif "ç§‘æŠ€åˆ›æ–°" in categories:
        parts.append("ğŸ’¡ **ç§‘æŠ€äº‹ä»¶æ´å¯Ÿ**ï¼šæŠ€æœ¯çªç ´éœ€åŒºåˆ†æ¦‚å¿µéªŒè¯ä¸å•†ä¸šè½åœ°ï¼Œå…³æ³¨ä¸“åˆ©ä¸äº§ä¸šé“¾æ•°æ®ã€‚\n")
    else:
        parts.append("ğŸ’¡ **æœ€ç»ˆæ´å¯Ÿ**ï¼šçœŸæ­£çš„ä¿¡æ¯ä¼˜åŠ¿ä¸åœ¨äºè·å–æ›´å¤šåŒç±»æŠ¥é“ï¼Œè€Œåœ¨äºåŒæ—¶æŒæ¡ä¸åŒè®¤çŸ¥æ¡†æ¶ï¼Œ"
                    "ä»è€Œåœ¨å¤æ‚ä¸–ç•Œä¸­å½¢æˆæ›´ç«‹ä½“ã€æ›´æŠ—åå·®çš„åˆ¤æ–­èƒ½åŠ›ã€‚\n")
    
    parts.append("\nğŸ“‹ **æ“ä½œæç¤º**ï¼šç‚¹å‡»ä¸Šæ–¹çš„'ğŸ”Š è¯»åˆ†æ'å’Œ'ğŸ”Š è¯»æ€»ç»“'æŒ‰é’®å¯å¬å–è¯­éŸ³ç‰ˆåˆ†æã€‚")
    
    return "".join(parts)


def build_triangle(sources):
    cn = sources.get("ä¸­å›½", [])
    us = sources.get("ç¾å›½ï¼ˆæ¬§ç¾åª’ä½“ï¼‰", [])
    aj = sources.get("ä¼Šæ–¯å…°ï¼ˆåŠå²›ç­‰ï¼‰", [])

    events = []
    seeds = [("ä¸­å›½", x) for x in cn] + [("ç¾å›½", x) for x in us] + [("ä¼Šæ–¯å…°", x) for x in aj]

    for src, seed in seeds:
        c = seed if src == "ä¸­å›½" else None
        u = seed if src == "ç¾å›½" else None
        a = seed if src == "ä¼Šæ–¯å…°" else None

        sc = 1 if c else 0
        su = 1 if u else 0
        sa = 1 if a else 0

        if c is None:
            c, sc = best_for(seed, cn)
        if u is None:
            u, su = best_for(seed, us)
        if a is None:
            a, sa = best_for(seed, aj)

        media_count = (1 if (c and sc > 0) else 0) + (1 if (u and su > 0) else 0) + (1 if (a and sa > 0) else 0)
        if media_count < 2:
            continue

        e = {
            "event_hint": seed["title"],
            "event_hint_zh": seed.get("title_zh", seed["title"]),
            "event_hint_en": seed.get("title_en", seed["title"]),
            "ä¸­å›½": c if (c and sc > 0) else None,
            "ç¾å›½": u if (u and su > 0) else None,
            "ä¼Šæ–¯å…°": a if (a and sa > 0) else None,
            "media_count": media_count,
            "score": sc + su + sa,
        }
        e["my_view"] = ai_view(e)
        e["summary"] = summary_view(e)
        events.append(e)

    uniq, seen = [], set()
    for e in sorted(events, key=lambda x: (x["media_count"], x["score"]), reverse=True):
        k = e.get("event_hint_zh", "")[:90]
        if k in seen:
            continue
        seen.add(k)
        uniq.append(e)

    return uniq[:MAX_TRIANGLE]


def main():
    payload = {
        "generated_at": datetime.now(TZ).strftime("%Y-%m-%d %H:%M:%S %Z"),
        "sources": {},
        "triangle": [],
        "errors": {},
    }

    for name, urls in SOURCES.items():
        merged, errs = [], []
        for url in urls:
            try:
                merged.extend(fetch_rss(url))
            except Exception as e:
                errs.append(f"{url}: {e}")

        seen, uniq = set(), []
        for it in merged:
            key = (it.get("title", ""), it.get("link", ""))
            if key in seen:
                continue
            seen.add(key)
            uniq.append(it)

        payload["sources"][name] = uniq[:MAX_CARD_ITEMS]
        if errs and not uniq:
            payload["errors"][name] = " | ".join(errs)

    payload["triangle"] = build_triangle(payload["sources"])

    OUT.parent.mkdir(parents=True, exist_ok=True)
    OUT.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"âœ… Updated: {OUT}")


if __name__ == "__main__":
    main()
